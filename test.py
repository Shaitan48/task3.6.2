# -*- coding: utf-8 -*-
"""task3.6.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JapQ3coY3VphPYsTK927yBpJHHhS9Rzk
"""

import random
import datetime
from random import randrange
from datetime import timedelta

#pip install pyspark

#pip install install-jdk

#pip install findspark

from pyspark.sql import SparkSession

from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType

def random_date(start, end):
    delta = end - start
    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds
    random_second = randrange(int_delta)
    return start + timedelta(seconds=random_second)

products = ['bread','beer','buter','bal','boots']
kolMin = 1
kolMax = 50
priceMin = 1
priceMax = 100000
dateMin = datetime.datetime(datetime.date.today().year,1,1,0,0,0)
dateMax = datetime.datetime.today()
genRecords = 50000

data = []
while genRecords > 0:
  data.append([random_date(dateMin, dateMax),random.randint(1,2000000000),random.choice(products), random.randrange(kolMax)+kolMin,random.randrange(priceMax)+priceMin])
  genRecords -= 1

spark = SparkSession.builder \
    .appName("task3.6.2") \
    .getOrCreate()

schema = StructType([
    StructField("date_order", DateType(), True),
    StructField("User_ID", IntegerType(), True),
    StructField("product", StringType(), True),
    StructField("kol", IntegerType(), True),
    StructField("price", IntegerType(), True)
])

df = spark.createDataFrame(data, schema)

df.repartition(1).write.format('com.databricks.spark.csv').save("/home/myuser/test2/test2/out.csv",header = 'true')
